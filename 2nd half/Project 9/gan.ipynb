{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Project9**\n",
    "\n",
    "Leader\n",
    "\n",
    "m5271046 Yuki Tome\n",
    "\n",
    "Member\n",
    "\n",
    "m5271022 Yusuke Matsushima\n",
    "\n",
    "m5271043 Hiroshi Tatsuta\n",
    "\n",
    "m5271009 Shota Higa"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 1: Understanding the GAN Algorithm\n",
    "\n",
    "Generative Adversarial Networks (GANs) consist of two main components: a generator and a discriminator. The generator creates new data instances, while the discriminator evaluates them for authenticity. The discriminator also checks parts of the real data set to ensure its accuracy. The goal of the generator is to generate passable data to fool the discriminator, and the discriminator's goal is to identify the generator's fakes.\n",
    "\n",
    "Here are the steps of the GAN algorithm:\n",
    "\n",
    "Initialize: Initialize the generator and discriminator networks. These can be any kind of neural network, but they are often Convolutional Neural Networks (CNNs).\n",
    "\n",
    "Generate Fake Data: The generator takes in random numbers and returns data (like an image). This data is called \"fake\".\n",
    "\n",
    "Discrimination: The discriminator takes in both real data (actual images from the dataset) and fake data (images produced by the generator). It returns probabilities, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake.\n",
    "\n",
    "Calculate Loss: Use the discriminator's probabilities to calculate the loss for both the generator and the discriminator. The generator's loss is calculated using the probabilities for the fake images, while the discriminator's loss is calculated using the probabilities for both the real and fake images.\n",
    "\n",
    "Backpropagation and Optimization: Use the gradients of the loss to update the generator and discriminator with gradient descent.\n",
    "\n",
    "Repeat: Repeat steps 2-5 for a certain number of epochs, or until the discriminator and generator loss does not improve significantly."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 2: Implementing a GAN Program\n",
    "\n",
    "I can provide you with a high-level overview of how to implement a GAN in Python using TensorFlow and Keras. However, due to the complexity of the code, I recommend following the detailed tutorials you provided for a step-by-step guide.\n",
    "\n",
    "Here is a high-level overview:\n",
    "\n",
    "Import Necessary Libraries: Import TensorFlow, Keras, and other necessary libraries.\n",
    "\n",
    "Load and Preprocess Data: Load your dataset (like MNIST) and preprocess it. This usually involves normalizing the data and reshaping it.\n",
    "\n",
    "Define the Generator: This is a neural network that takes in a random noise signal and outputs an image. It usually involves a series of dense and reshape layers.\n",
    "\n",
    "Define the Discriminator: This is a neural network that takes in an image and outputs a probability that the image is real. It usually involves a series of convolutional layers.\n",
    "\n",
    "Define the GAN: This involves connecting the generator and discriminator networks. You will also define the loss function and optimizers for both networks.\n",
    "\n",
    "Train the GAN: This involves generating images with the generator, training the discriminator with both real and fake images, and then training the generator to try to fool the discriminator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part 3: Evaluating the Program\n",
    "\n",
    "After training the GAN, you can evaluate it by generating new images and visually inspecting them. You can also compare the GAN's performance to other generative models like Autoencoders (AE) and Restricted Boltzmann Machines (RBM).\n",
    "\n",
    "Here are some differences between these models:\n",
    "\n",
    "GANs generate new data instances that are similar to your training data. They are great for generating new images, music, and even text.\n",
    "\n",
    "Autoencoders are used for learning efficient codings of input data. They can be used for dimensionality reduction, or for learning to reconstruct input data. However, they are not explicitly designed to generate new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-05 10:23:28.952411: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Flatten, Reshape, LeakyReLU, Dropout, Input\n",
    "from keras.optimizers import Adam\n",
    "from keras import initializers\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "X_train = (X_train.astype(np.float32) - 127.5)/127.5\n",
    "X_train = X_train.reshape(60000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_dim = 100\n",
    "\n",
    "def create_generator():\n",
    "    generator = Sequential()\n",
    "    generator.add(Dense(256, input_dim=random_dim, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(Dense(512))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(Dense(1024))\n",
    "    generator.add(LeakyReLU(0.2))\n",
    "    generator.add(Dense(784, activation='tanh'))\n",
    "    return generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_discriminator():\n",
    "    discriminator = Sequential()\n",
    "    discriminator.add(Dense(1024, input_dim=784, kernel_initializer=initializers.RandomNormal(stddev=0.02)))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    discriminator.add(Dense(512))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    discriminator.add(Dense(256))\n",
    "    discriminator.add(LeakyReLU(0.2))\n",
    "    discriminator.add(Dropout(0.3))\n",
    "    discriminator.add(Dense(1, activation='sigmoid'))\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gan(discriminator, random_dim, generator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=(random_dim,))\n",
    "    x = generator(gan_input)\n",
    "    gan_output = discriminator(x)\n",
    "    gan = Model(inputs=gan_input, outputs=gan_output)\n",
    "    gan.compile(loss='binary_crossentropy', optimizer=Adam())\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_gan(gan, discriminator, generator, random_dim, epochs=1, batch_size=128):\n",
    "    for e in range(1, epochs+1):\n",
    "        for _ in tqdm(range(batch_size)):\n",
    "            noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "            image_batch = X_train[np.random.randint(0, X_train.shape[0], size=batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate fake MNIST images\n",
    "generated_images = create_generator().predict(noise)\n",
    "X = np.concatenate([image_batch, generated_images])\n",
    "\n",
    "# Labels for generated and real data\n",
    "y_dis = np.zeros(2*batch_size)\n",
    "# One-sided label smoothing\n",
    "y_dis[:batch_size] = 0.9\n",
    "\n",
    "# Train discriminator\n",
    "discriminator.trainable = True\n",
    "discriminator.train_on_batch(X, y_dis)\n",
    "\n",
    "# Train generator\n",
    "noise = np.random.normal(0, 1, size=[batch_size, random_dim])\n",
    "y_gen = np.ones(batch_size)\n",
    "discriminator.trainable = False\n",
    "gan.train_on_batch(noise, y_gen)\n",
    "\n",
    "if e == 1 or e % 20 == 0:\n",
    "    print(f\"Epoch {e} of {epochs}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
